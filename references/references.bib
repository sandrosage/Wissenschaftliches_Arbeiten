% This file was created with Citavi 6.14.0.0

@article{Boutaba.2018,
 abstract = {Machine Learning (ML) has been enjoying an unprecedented surge in applications that solve problems and enable automation in diverse domains. Primarily, this is due to the explosion in the availability of data, significant improvements in ML techniques, and advancement in computing capabilities. Undoubtedly, ML has been applied to various mundane and complex problems arising in network operation and management. There are various surveys on ML for specific areas in networking or for specific network technologies. This survey is original, since it jointly presents the application of diverse ML techniques in various key areas of networking across different network technologies. In this way, readers will benefit from a comprehensive discussion on the different learning paradigms and ML techniques applied to fundamental problems in networking, including traffic prediction, routing and classification, congestion control, resource and fault management, QoS and QoE management, and network security. Furthermore, this survey delineates the limitations, give insights, research challenges and future opportunities to advance ML in networking. Therefore, this is a timely contribution of the implications of ML for networking, that is pushing the barriers of autonomic network operation and management.},
 author = {Boutaba, Raouf and Salahuddin, Mohammad A. and Limam, Noura and Ayoubi, Sara and Shahriar, Nashid and Estrada-Solano, Felipe and Caicedo, Oscar M.},
 year = {2018},
 title = {A comprehensive survey on machine learning for networking: evolution, applications and research opportunities},
 url = {https://link.springer.com/article/10.1186/s13174-018-0087-2},
 keywords = {Computer Applications;Computer Communication Networks;Computer Systems Organization and Communication Networks;Information Systems and Communication Service;IT in Business;Processor Architectures},
 pages = {1--99},
 volume = {9},
 number = {1},
 issn = {1867-4828},
 journal = {Journal of Internet Services and Applications},
 doi = {10.1186/s13174-018-0087-2},
 file = {A comprehensive survey on machine learning for networkin:Attachments/A comprehensive survey on machine learning for networkin.pdf:application/pdf}
}


@article{Breiman.1996,
 abstract = {Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy.},
 author = {Breiman, Leo},
 year = {1996},
 title = {Bagging predictors},
 url = {https://link.springer.com/article/10.1007/BF00058655},
 keywords = {Artificial Intelligence;Control;Mechatronics;Natural Language Processing (NLP);Robotics;Simulation and Modeling},
 pages = {123--140},
 volume = {24},
 number = {2},
 issn = {0885-6125},
 journal = {Machine Learning},
 doi = {10.1007/BF00058655},
 file = {Bagging predictors:Attachments/Bagging predictors.pdf:application/pdf}
}


@incollection{Dietterich.,
 author = {Dietterich, Thomas G.},
 title = {Ensemble Methods in Machine Learning},
 pages = {1--15},
 volume = {1857},
 doi = {10.1007/3-540-45014-9_1},
 file = {Ensemble Methods in Machine Learning:Attachments/Ensemble Methods in Machine Learning.pdf:application/pdf}
}


@book{Kubat.2021,
 author = {Kubat, Miroslav},
 year = {2021},
 title = {An Introduction to Machine Learning},
 address = {Cham},
 publisher = {{Springer International Publishing}},
 isbn = {978-3-030-81934-7},
 doi = {10.1007/978-3-030-81935-4},
 file = {An Introduction to Machine Learning:Attachments/An Introduction to Machine Learning.pdf:application/pdf}
}


@book{Zhou.2012,
 abstract = {An up-to-date, self-contained introduction to a state-of-the-art machine learning approach, Ensemble Methods: Foundations and Algorithms shows how these},
 author = {Zhou, Zhi-Hua},
 year = {2012},
 title = {Ensemble Methods},
 url = {https://www.taylorfrancis.com/books/mono/10.1201/b12207/ensemble-methods-zhi-hua-zhou},
 publisher = {{Chapman and Hall/CRC}},
 isbn = {9780429151095},
 doi = {10.1201/b12207},
 file = {Ensemble Methods  Foundations and Algorithms:Attachments/Ensemble Methods  Foundations and Algorithms.pdf:application/pdf}
}


@book{Zhou.2021,
 author = {Zhou, Zhi-Hua},
 year = {2021},
 title = {Machine Learning},
 address = {Singapore},
 publisher = {{Springer Singapore}},
 isbn = {978-981-15-1966-6},
 doi = {10.1007/978-981-15-1967-3},
 file = {Machine Learning:Attachments/Machine Learning.pdf:application/pdf}
}

% This file was created with Citavi 6.14.0.0

@article{Freund.1997,
 author = {Freund, Yoav and Schapire, Robert E.},
 year = {1997},
 title = {A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting},
 pages = {119--139},
 volume = {55},
 number = {1},
 issn = {00220000},
 journal = {Journal of Computer and System Sciences},
 doi = {10.1006/jcss.1997.1504},
 file = {Freund, Schapire 1997 - A Decision-Theoretic Generalization of On-Line:Attachments/Freund, Schapire 1997 - A Decision-Theoretic Generalization of On-Line.pdf:application/pdf}
}

% This file was created with Citavi 6.14.0.0

@book{kohavi.1996,
 year = {1996},
 title = {Bias plus variance decomposition for zero-one loss functions},
 url = {http://robotics.stanford.edu/users/ronnyk/biasvar.pdf},
 file = {Bias plus variance decomposition 1996:Attachments/Bias plus variance decomposition 1996.pdf:application/pdf}
}








